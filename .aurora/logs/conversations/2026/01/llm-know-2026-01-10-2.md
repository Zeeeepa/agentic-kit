# SOAR Conversation Log

**Query ID**: soar-1768061574165
**Timestamp**: 2026-01-10T17:13:17.346618
**User Query**: how does LLM know when to invoke CoT or ToT?

---

## Execution Summary

- **Duration**: 23179.48269844055ms
- **Overall Score**: 0.90
- **Cached**: False
- **Cost**: $0.0066
- **Tokens Used**: 23 input + 434 output

## Metadata

```json
{
  "query_id": "soar-1768061574165",
  "query": "how does LLM know when to invoke CoT or ToT?",
  "total_duration_ms": 23179.48269844055,
  "total_cost_usd": 0.006578999999999999,
  "tokens_used": {
    "input": 23,
    "output": 434
  },
  "budget_status": {
    "period": "2026-01",
    "limit_usd": 10.0,
    "consumed_usd": 0.16570499999999994,
    "remaining_usd": 9.834295000000001,
    "percent_consumed": 1.6570499999999995,
    "at_soft_limit": false,
    "at_hard_limit": false,
    "total_entries": 152
  },
  "phases": {
    "phase1_assess": {
      "complexity": "SIMPLE",
      "confidence": 0.8772727272727274,
      "method": "keyword",
      "reasoning": "Keyword-based classification with 87.7% confidence",
      "score": 0.02,
      "_timing_ms": 20.417213439941406,
      "_error": null
    },
    "phase2_retrieve": {
      "code_chunks": [],
      "reasoning_chunks": [],
      "total_retrieved": 0,
      "high_quality_count": 0,
      "retrieval_time_ms": 0.04673004150390625,
      "budget": 5,
      "budget_used": 0,
      "_timing_ms": 0.07462501525878906,
      "_error": null
    },
    "phase7_synthesize": {
      "answer": "LLMs don't automatically \"know\" when to invoke Chain-of-Thought (CoT) or Tree-of-Thought (ToT) reasoning - these are **prompting techniques** controlled by either:\n\n## 1. **Explicit Prompting** (Most Common)\nThe user or system designer explicitly requests it:\n\n```\n# Chain-of-Thought\n\"Solve this step-by-step, showing your reasoning\"\n\"Let's think through this carefully\"\n\n# Tree-of-Thought\n\"Consider multiple approaches and evaluate each\"\n\"Explore different solution paths before deciding\"\n```\n\n## 2. **System Instructions**\nThe model's system prompt can include instructions to use these techniques for certain task types:\n\n```markdown\nWhen solving complex problems:\n- Use step-by-step reasoning (CoT)\n- For multi-step planning, evaluate alternatives (ToT)\n```\n\n## 3. **Framework/Tool Design**\nSome frameworks implement these patterns programmatically:\n\n- **LangChain/LangGraph**: Explicitly defines reasoning chains\n- **ReAct**: Structures Reasoning + Acting loops\n- **ToT Libraries**: Force branching exploration with scoring functions\n\n## 4. **Trained Behavior** (Limited)\nSome newer models (GPT-4, Claude 3+) have been fine-tuned to recognize problem complexity and engage deeper reasoning when:\n- Questions are ambiguous or multi-faceted\n- Explicit uncertainty markers appear (\"I'm not sure\", \"What's the best way\")\n- Math/logic problems are detected\n\nHowever, this is **unreliable without explicit prompting**.\n\n## Key Point\nLLMs are **passive** - they don't autonomously choose reasoning strategies. CoT/ToT must be:\n1. Requested in the prompt\n2. Enforced by system instructions\n3. Structured by the application layer\n\nThe model responds to the pattern you establish, not from independent decision-making about reasoning methods.\n",
      "confidence": 0.9,
      "traceability": [],
      "metadata": {
        "simple_path": true,
        "context_used": {
          "code_chunks": 0,
          "reasoning_chunks": 0
        }
      },
      "timing": {
        "synthesis_ms": 23145.7040309906,
        "input_tokens": 23,
        "output_tokens": 434
      }
    },
    "phase8_record": {
      "cached": false,
      "reasoning_chunk_id": null,
      "pattern_marked": false,
      "activation_update": 0.0,
      "timing": {
        "record_ms": 0
      }
    },
    "phase9_respond": {
      "verbosity": "normal",
      "formatted": true
    }
  },
  "timestamp": 1768061597.3455389
}
```
